{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests & Ensembles\n",
    "\n",
    "Nesse notebook vamos estudar técnicas para combinar o resultado de múltiplos modelos de forma a construir preditores mais poderosos e mitigar alguns dos problemas que surgem nos métodos que já estudamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8.0, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relembrando sobre Árvores de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, target_names = utils.load_dataset('cancer')\n",
    "\n",
    "print('instancias X features:', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ytrain_pred = dt.predict(xtrain)\n",
    "ytest_pred = dt.predict(xtest)\n",
    "\n",
    "print('Acurácia no treino:', accuracy_score(ytrain, ytrain_pred))\n",
    "print('Acurácia no teste:', accuracy_score(ytest, ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "dot_data = export_graphviz(dt,\n",
    "                           out_file=None,\n",
    "                           feature_names=x.columns,\n",
    "                           class_names=target_names,\n",
    "                           filled=True, rounded=True, \n",
    "                           special_characters=True)\n",
    "graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtivemos uma acurácia no teste de 94%, o que parece bem razoável. Vamos tentar de novo, mas vamos tirar o `random_state=42` do nosso `train_test_split`, o que vai fazer com que os nossos 150 exemplos de teste (e consequentemente os nossos exemplos de treino) sejam sempre diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=150)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(xtrain, ytrain)\n",
    "\n",
    "accuracy_score(ytest, dt.predict(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fronteiras de Decisão\n",
    "\n",
    "Podemos visualizar as fronteiras de decisão de cada classificador treinado para entender melhor os erros que eles cometem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = x[['mean concave points', 'worst perimeter']]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(sx, y, test_size=150)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(xtrain, ytrain)\n",
    "    \n",
    "utils.plot2d(sx.values, y.values, clf=dt)\n",
    "plt.xlim((-0.01, 0.20))\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(xtrain, ytrain)\n",
    "\n",
    "utils.plot2d(sx.values, y.values, clf=lr)\n",
    "plt.xlim((-0.01, 0.20))\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining (ensembling)\n",
    "\n",
    "Temos vários classificadores treinados com árvores de decisão. Cada um deles é enviesado de uma forma diferente. Por que não combinar a opinião de todos eles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x, y, f):\n",
    "    sx = x.sample(frac=f)\n",
    "    sy = y.loc[sx.index]\n",
    "    return sx, sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=100)\n",
    "\n",
    "accs = []\n",
    "predictions = []\n",
    "for r in range(100):\n",
    "    sx, sy = sample(xtrain, ytrain, 0.2)\n",
    "    ...\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(accs))\n",
    "\n",
    "# combine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A técnica que acabamos de usar é um tipo de **ensembling**, mais especificamente um tipo de **bagging**. Nessa aula vamos aprender outras estratégias e formas mais fáceis de implementá-las."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "É muito simples utilizar random forest no nosso dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = rf.predict(xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício: Inadimplência\n",
    "\n",
    "Nesse exemplo vamos carregar um dataset novo. Cada exemplo é uma pessoa que pediu uma linha de crédito e devemos decidir a probabilidade dessa pessoa nos pagar de volta, ou não. Algumas coisas que vocês devem fazer:\n",
    "\n",
    "* Liste os atributos do problema.\n",
    "* Observe se algum atributo é categório (não numérico).\n",
    "* Observe se algum atributo tem dados faltantes (nulls e nans).\n",
    "* Observe se o problema é balanceado (o que é isso mesmo?).\n",
    "* Separe um conjunto de teste e um de treino (quanto pra cada?)\n",
    "* Treine uma árvore de decisão, uma regressão logística e uma floresta aleatória.\n",
    "* Utilizem a função `eval_auc` para estimar qual dos modelos acima é melhor.\n",
    "* Observe o desempenho ao experimentar com o parâmetro `max_depth` (10, 20, 30, etc). Repare no tempo de execução também.\n",
    "* Use o parâmetro `n_estimators` de `RandomForestClassifier` para ver como isso impacta o desempenho do seu modelo.\n",
    "* Plote um gráfico `n_estimators` x `AUC` para sua random forest.\n",
    "* A partir de qual valor de `n_estimators` você acha que não vale mais a pena aumentar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def eval_auc(clf, x, y):\n",
    "    ypred = clf.predict_proba(x)[:,1]\n",
    "    return roc_auc_score(y, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = utils.load_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exercício vamos observar problemas de overfitting e underfitting e tentar criar ensembles simples parar mitigar tais problemas. Iremos juntos:\n",
    "\n",
    "* Plotar os dados e observar a correlação entre x e y\n",
    "* Utilizar `DecisionTreeRegressor` para treinar e visualizar um regressor nesses dados\n",
    "* Experimentar com alguns parâmetros diferentes e observar os efeitos nos modelos\n",
    "* Iterar N vezes para treinar N modelos diferentes com diferentes amostras dos dados\n",
    "* Combinar os resultados de todos os modelos e observar o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x, y, n):\n",
    "    idx = np.random.randint(len(x), size=n)\n",
    "    return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, y = utils.load_dataset('regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "train_sample = 0.5\n",
    "\n",
    "plt.plot(x, y, '.', label=\"data\")\n",
    "\n",
    "#...\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exercício vamos experimentar usar a `BaggingClassifier` do sklearn para criar ensembles com regressão logística e árvores de decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = utils.load_dataset('default')\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "n_estimators = [1,2,3,4,5,10,15,20]\n",
    "dt_bag_scores = []\n",
    "lr_bag_scores = []\n",
    "for ne in n_estimators:\n",
    "    dt = DecisionTreeClassifier(max_depth=15, random_state=1)\n",
    "    lr = LogisticRegression(random_state=1)\n",
    "    \n",
    "    dt_bag = BaggingClassifier(dt, n_estimators=ne)\n",
    "    lr_bag = BaggingClassifier(lr, n_estimators=ne)\n",
    "\n",
    "    dt_bag.fit(xtrain, ytrain)\n",
    "    lr_bag.fit(xtrain, ytrain)\n",
    "\n",
    "    dt_bag_scores.append(eval_auc(dt_bag, xtest, ytest))\n",
    "    lr_bag_scores.append(eval_auc(lr_bag, xtest, ytest))\n",
    "\n",
    "    print(ne, dt_bag_scores[-1], lr_bag_scores[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_estimators, dt_bag_scores, label='bagging')\n",
    "plt.plot(n_estimators, lr_bag_scores, label='forest')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse resultado talvez deixe claro por que *random forests* são tão populares e não *random logistic regressions* não :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "\n",
    "Vamos usar o `VotingClassifier` do `sklearn` para compor alguns classificadores diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = utils.load_dataset('default')\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "dt1 = DecisionTreeClassifier(random_state=1)\n",
    "dt2 = DecisionTreeClassifier(max_depth=12, max_features=5, random_state=42)\n",
    "lr1 = LogisticRegression()\n",
    "lr2 = LogisticRegression(C=0.001, penalty='l1')\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "clfs = [('dt1', dt1),\n",
    "        ('dt2', dt2),\n",
    "        ('lr1', lr1),\n",
    "        ('lr2', lr2),\n",
    "        ('knn', knn)]\n",
    "\n",
    "vot = VotingClassifier(clfs, voting='soft')\n",
    "vot.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in clfs:\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    \n",
    "    auc = eval_auc(clf, xtest, ytest)\n",
    "    print(name, auc)\n",
    "    \n",
    "print()\n",
    "print('all', eval_auc(vot, xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-competição!\n",
    "\n",
    "Vamos usar os métodos que aprendemos pra tentar melhorar nosso resultado nesse dataset de inadimplência! Usem a criatividade :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = utils.load_dataset('default')\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(xtrain, ytrain)\n",
    "\n",
    "eval_auc(clf, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Vamos usar os mesmos estimadores base do exemplo anterior, porém ao invés de definir a predição por voto, vamos **treinar outro classificador com as predições dos classificadores base**. Primeiro começamos com nosso split train/test padrão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora precisamos fazer um novo split no conjunto de treino para reservar dados para nosso meta-learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbase, xmeta, ybase, ymeta = train_test_split(xtrain, ytrain, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmeta_preds = pd.DataFrame(index=ymeta.index)\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_clf = DecisionTreeClassifier(max_depth=7, random_state=1)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_meta = pd.DataFrame(index=ytest.index)\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_auc(meta_clf, xtest_meta, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De volta ao Kaggle!\n",
    "\n",
    "![Comp](taxi-competition.png)\n",
    "\n",
    "Vamos utilizar as técnicas que aprendemos para treinar modelos pra prever quanto tempo uma viagem de taxi em NY vai levar. Depois vamos submeter nossa solução para o Kaggle.\n",
    "\n",
    "Dessa vez é pra ganhar!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/kaggle/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Instances x features:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que já vimos que não precisamos imputar (han??) nenhuma feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porém precisamos lidar com algumas variáveis não numéricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma relembrada rápida na distribuição nosso target (tempo de viagem):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.trip_duration.hist(bins=range(0,5000,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar nossas features e nosso target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['trip_duration', 'id', 'pickup_datetime', 'dropoff_datetime', 'store_and_fwd_flag'], axis=1)\n",
    "y = df['trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=50)\n",
    "reg.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_log_error\n",
    "\n",
    "def rmsle(ytrue, ypred):\n",
    "    return np.sqrt(mean_squared_log_error(ytrue, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = reg.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(ytest, ypred)/60)\n",
    "print(rmsle(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(ytest, ypred)/60)\n",
    "print(rmsle(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Para montar nossa submissão precisamos:\n",
    "\n",
    "* Carregar o arquivo CSV\n",
    "* Aplicar o mesmo pre-processamento que fizemos no treino (que ainda é bem simples)\n",
    "* Fazer as predições\n",
    "* Montar e salvar um arquivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv('../data/kaggle/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sub = df_sub.drop(['id', 'pickup_datetime', 'store_and_fwd_flag'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub = reg.predict(x_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'id': df_sub.id, 'trip_duration': y_sub})\n",
    "sub.to_csv('../data/kaggle/sub1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
