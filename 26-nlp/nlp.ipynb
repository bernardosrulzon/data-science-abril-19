{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aula 26 - NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media1.tenor.com/images/2c5e2003710c3f8e3d39ebb424b80f0c/tenor.gif?itemid=6169062)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://everythingbutthebooks.files.wordpress.com/2014/06/gif-rapunzel.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Campo relativamente novo da computação que combina ML e linguística.\n",
    "- Principal foco: fazer as máquinas entendam (e até se comuniquem) em linguagem humana. \n",
    "- Área de pesquisa/atuação extremamente ampla.\n",
    "- Biggest consequence: lowering (or complete removal) of the barrier to entry for BI and big data in general: \n",
    "\n",
    "> \"Google might tell you today what the weather will be tomorrow. But soon enough, you’ll be able to ask your personal data chatbot about customer sentiment today, and how they’ll feel about your brand next week; all while walking down the street.\"\n",
    "> https://www.sisense.com/blog/heres-natural-language-processing-future-bi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nlp-diagram.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exemplos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nlp-fields.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                       - Autocomplete: celular, e-mail, pesquisa Google.\n",
    "                       - Spell checker\n",
    "                       - Spam detection\n",
    "                       - Information Retrieval: query do usuário -> produto/documento (Google)\n",
    "                       - Chatbot\n",
    "                       - Q&A: pergunta -> resposta (Watson/Jeopardy)\n",
    "                       - Speech recognition\n",
    "                       - Machine Translation (Google Translate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os dados: \n",
    "Suponha que você é um Cientista de Dados, trabalha em um app de um restaurante e tem reviews de alguns clientes.\n",
    "\n",
    "*PS: Esse conjunto de dados é composto de reviews de restaurantes e foi modificado a partir do dataset usado no workshop SemEval (International Workshop on Semantic Evaluation) na [edição de 2016](http://alt.qcri.org/semeval2016/task5/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from collections import Counter\n",
    "from pycontractions import Contractions\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from utils.confusion_matrix import plot_confusion_matrix\n",
    "from utils.to_dense import DenseTransformer\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/raw/raw_reviews.csv').sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual a tarefa??\n",
    "\n",
    "Como agregar valor ao negócio a partir desses dados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa: *Sentiment analysis*\n",
    "Classificar review em positiva ou negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/processed/train.csv')\n",
    "test_df = pd.read_csv('data/processed/test.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As ferramentas\n",
    "- Principal biblioteca atualmente para trabalhar com NLP: *Spacy*. \n",
    "\n",
    "* O datacamp lançou recentemente um [curso](https://campus.datacamp.com/courses/advanced-nlp-with-spacy) bem legal sobre ele que vale a pena dar uma conferida já que hoje, focaremos no \"básico\".\n",
    "\n",
    "- Outra lib bastante usada (principal biblioteca até alguns meses atrás): [NLTK](https://www.nltk.org/), .\n",
    "\n",
    "- Outras iniciativas famosas com deep learning: [AllenNLP](https://allennlp.org/) e o [StanfordNLP](https://stanfordnlp.github.io/stanfordnlp/), que são capazes de atingir o estado da arte de muitas aplicações.\n",
    "\n",
    "\n",
    "#### Spacy: \n",
    "- Conceito de objeto (comumente chamada de 'nlp') que contém todo o pipeline de processamento, além de outras regras específicas de uma certa língua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como a máquina vai \"entender\" os textos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://img.youtube.com/vi/d4gGtcobq8M/0.jpg)](http://www.youtube.com/watch?v=d4gGtcobq8M \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    **Patterns, Disambiguation (context), Stopwords, Tokens, Bag-of-words, TF, TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline NLP**\n",
    "<img src=\"images/nlp-pipe.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patterns\n",
    "Em alguns casos, pode ser interessante substituir certas partes em textos fixos, como horários, quantidades, emails.\n",
    "\n",
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(u'[a-z0-9\\_\\.\\-]*@[a-z0-9\\_\\.\\-]*', 'EMAIL_ADDR', u'hi there thais.neubauer@gmail.com!',flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "- Segmentação de sentenças em \"palavras\".\n",
    "- Análise léxica: eliminação de caracteres de pontuação e outros caracteres \"especiais\" - não alfa-numéricos (em alguns casos, excluindo até mesmo os numéricos e acentuação).\n",
    "\n",
    "Exemplos de potenciais problemas:\n",
    "\n",
    "<img src=\"images/nlp-tokenizing_problems.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"We went around 9:30 on a Friday...\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"It's also attached to Angel's Share, which is a cool, more romantic bar...\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercício: Crie o objeto nlp para português e imprima o texto do primeiro token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Portuguese language class\n",
    "from spacy.lang.___ import ________\n",
    "\n",
    "# Create the nlp object\n",
    "nlp_pt = ___________\n",
    "\n",
    "# Process a text\n",
    "doc = nlp_pt(\"Isso é uma sentença\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = _______\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token._______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords (and maybe other tokens)\n",
    "- Para muitas tarefas de NLP, é bom prestar atenção nas chamadas *stopwords*, que são as palavras muito comuns que aparecem no texto e que, por isso, não tem potencial para contribuir para a caracterização do conteúdo presente no texto. \n",
    "\n",
    "- Nessa lista geramente estão: artigos definidos e indefinidos, preposições, pronomes, numerais, conjunções e advérbios. \n",
    "\n",
    "-> IMPORTANTE: Além das palavras pertencentes a essas classes gramaticais, podem entrar na lista as palavras muito comuns dentro do contexto referente aos documentos do corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lei de Zipf e proposta de cortes de Luhn:\n",
    "\n",
    "<img src=\"images/nlp-freq_import.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#splita as frases por palavras (espaco incluido) e soma elas\n",
    "Counter(sum(train_df['text'].str.lower().str.split(r'[\\W\\s]+').tolist(), [])).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "O que essa regex está fazendo é basicamente convertendo todo o texto para minúsculo e depois 'splitando' em tokens sempre que o texto encontra um caracter em brando (\\s) **ou** sempre que encontra um caracter que **não** é alfanumérico ([^a-zA-Z0-9]), representado pelo \\W\n",
    "\n",
    "Depois, ela pega as 10 ocorrências mais comumns.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No spacy já existe uma lista de stopwords comuns, que podemos ver assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "en_stopwords = sorted([token.text for token in nlp.vocab if token.is_stop])\n",
    "en_stopwords[155:161]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercício:\n",
    "Explore um pouco a lista e analise as que você acha que podem ser excluídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_excl = ['cannot', 'go', 'off']\n",
    "\n",
    "for w in list_excl:\n",
    "    nlp.vocab[w].is_stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificarmos quais palavras excluímos da lista de stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' -- '.join([w for w in en_stopwords if not nlp.vocab[w].is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Além de excluir, podemos também incluir, pois, como dito no início da conversa sobre *stopwords*, podem entrar na lista as palavras muito comuns dentro do contexto referente aos documentos do corpus.\n",
    "- Outra forma de **definir stopwords** é **verificando a frequência das palavras no corpus**, ou seja, contando a frequência de cada token no *corpus*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Normalize words into its base or root form, using common prefixes and suffixes.\n",
    "\n",
    "Exemplo: affectation, affects, affections, affected, affection, affecting -> affect.\n",
    "\n",
    "Problemas: boiando -> boi, factual -> fact (suffixe \"ual\"), equal -> eq (suffixe \"ual\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Also maps several similar words into one common root, but groups together different inflected forms of a word to its *lemma* (dictionary needed), which is a proper word. \n",
    "\n",
    "Exemplo: gone, going, went -> go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-Of-Speech (POS) Tags\n",
    "Grammatical type of the word: noun, verb, adjective, adverb, pronoun, preposition, conjunction, determiner, exclamation.\n",
    "\n",
    "Exemplo: \n",
    "\n",
    "<img src=\"images/nlp-pos.png\" width=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Objeto \"Token\" - Spacy\n",
    "\n",
    "É possível carregar alguns modelos pré treinados no Spacy para prever atributos linguísticos, como:\n",
    "\n",
    "- POS tags (classificação gramátical - Apple companhia vs apple fruta)\n",
    "- Nomeação de Entidades (Apple companhia, Photograph Nickelback album)\n",
    "\n",
    "\n",
    "Tais modelos estão divididos em pacotes que precisam ser baixados, como o 'en_core_web_sm', pacote com vários modelos treinados em inglês.\n",
    "\n",
    "\n",
    "O objeto Token é um objeto que possui vários atributos, inclusive o \"pos_\" . Outros atributos importantes/legais:\n",
    "\n",
    "<img src=\"images/spacy.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I heard Photograph from Nicklelback yesterday !\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercício: Se usarmos .lower() no texto abaixo, como ficam as entidades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalização:\n",
    "\n",
    "- Além do que já comentamos até aqui, outras diferentes formas de escrever a mesma coisa podem \"confundir\" os algoritmos. \n",
    "\n",
    "- Transformar o texto todo em minúsculo e retirar acentos, pro exemplo, são bastante comuns. \n",
    "\n",
    "CUIDADO pra essas etapas não atrapalharem outras, como identificação de entidades!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contractions \n",
    "Outro exemplo de normalização interessante na língua inglesa é a expansão de contrações: I'm -> I am, We're -> we are etc.\n",
    "\n",
    "Para expansão de contractions, usaremos o [PyContractions](https://pypi.org/project/pycontractions/) e especificaremos o modelo da api gensim.downloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "list(cont.expand_texts([\"We're all so happy!! Can't believe this!\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etapas do pré-processamento:\n",
    "\n",
    "\n",
    "1. Remove entidades (remove_ents);\n",
    "2. Expande constrações (expand_contractions);\n",
    "3. Transforme tudo para minúscula;\n",
    "4. Remove stopwords e pontuações (remove_stop_and_punct).\n",
    "\n",
    "IMPORTANTE: Atenção na ordem!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercício: Crie uma função ```preprocess``` que recebe qualquer string como entrada e faz todas as etapas do pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ents(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        text = text.replace(ent.text, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return list(________________)[0]\n",
    "\n",
    "        \n",
    "def remove_stop_and_punct(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if _________________:\n",
    "            continue\n",
    "        tokens.append(token.text)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(original_text):\n",
    "    new_text = remove_ents(original_text)\n",
    "    new_text = expand_contractions(new_text)\n",
    "    if len(new_text) == 0:\n",
    "        return new_text \n",
    "    return remove_stop_and_punct(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ja fizemos isso neste notebook, mas aqui teriamos que carregar esses objetos se já não estivéssemos feito ainda, \n",
    "#pois as funções estão usando-os. Porém, atenção: carregamos E alteramos o en_core_web_sm quando exluímos algumas \n",
    "#palavras da lista de stopwords, então se carregarmos novamente, temos que excluir as stopwords novamente também\n",
    "\n",
    "#nlp = English()\n",
    "#nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(preprocess(\"It's also not attached to Angel's Share, which is a cool, more romantic bar...\") \n",
    "       == 'not attached cool romantic bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercício: Aplique sua função preprocess à coluna text (dos dois dataframes, train_df e test_df), criando uma coluna nova norm_text, que será usada para treinarmos um modelo de análise de sentimento. Dica: Use o apply do pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['norm_text'] = train_df['text'].apply(preprocess)\n",
    "train_df.to_pickle('data/processed/train_preprocessed.pickle')\n",
    "print(\"Train:\\n\")\n",
    "display(train_df[['text','norm_text','polarity']].sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['norm_text'] = test_df['text'].apply(preprocess)\n",
    "test_df.to_pickle('data/processed/train_preprocessed.pickle')\n",
    "print(\"Test:\\n\")\n",
    "display(test_df[['text','norm_text','polarity']].sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words\n",
    "\n",
    "<img src=\"images/nlp-bow.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Com o Scikit Learn: [CountVectorizer](https://scikit-learn.org/0.19/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_for_bow = [\n",
    "    'camisa preta botao botao botao',\n",
    "    'botao feito linha preta',\n",
    "    'considera-se caro preco botao camisa botao',\n",
    "    'linha costurar botão mesma camisa',\n",
    "    'costurar linha camisa mesma botao'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(strip_accents='unicode', binary=True)\n",
    "bow_matrix = cv.fit_transform(examples_for_bow)\n",
    "columns = [token[0] for token in sorted(cv.vocabulary_.items(), key=lambda item: item[1])]\n",
    "pd.DataFrame(bow_matrix.todense(), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercício: Use o parâmetro 'max_features' do CountVectorizer para diminuir a quantidade de dimensões e veja a diferença na matriz resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF: E se usássemos a quantidade de vezes que o token ocorre?\n",
    "<img src=\"images/nlp-bow2.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercício: usando o [CountVectorizer](https://scikit-learn.org/0.19/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) mesmo, como obter a matriz TF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "<img src=\"images/nlp-tfidf.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercício: use o [TfidfVectorizer](https://scikit-learn.org/0.19/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) para vetorizar os mesmos dados dos outros vetorizadores que estamos usando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "<img src=\"images/nlp-ngrams.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados estão prontos para \"enviar\" pro modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não, ne? Próximos passos:\n",
    "1. Target de string pra inteiro.\n",
    "2. Separar X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercício: transforme a coluna polarity em inteiro (0 se \"negative\" e 1 se \"positive\") e verifique em um sample o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['y'] = ____________\n",
    "train_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['y'] = ____________\n",
    "test_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['norm_text'].values\n",
    "y_train = train_df['y'].values\n",
    "X_test = test_df['norm_text'].values\n",
    "y_test = test_df['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercício: monte um [Pipeline](https://scikit-learn.org/0.19/modules/generated/sklearn.pipeline.Pipeline.html) com o CountVectorizer binário e LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('vect', __________),\n",
    "    ('clf', ___________)\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.________([\"It's also attached to Angel's Share, which is a cool, more romantic bar...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.________([\"It's also attached to Angel's Share, which is a cool, more romantic bar...\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercício: Avalie o resultado com a *classification_report* e com a *plot_confusion_matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['negative', 'positive']\n",
    "classification_report(______, ______, target_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(________, ________);\n",
    "\n",
    "# Plot normalized confusion matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
